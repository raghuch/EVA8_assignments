This folder hosts the solution for assignment 4 of the EVA-8 course.Please click the first link of each paragraph to go to the corresponding notebook.

Steps in obtaining 99.4% test accuracy:

[Step 1](https://github.com/raghuch/EVA8_assignments/blob/main/assignment4/EVA4S5F2_Step1.ipynb) is a naive code taken directly from the class, just to see how well the CNN performs, without paying attention to the model size. We just take the MNIST data and convert to Tensor and normalize it (no transforms). It has close to 195k model params which are 19.5X our ideal size, but we get a max test accuracy of 98.86 in 20 epochs and 98.72 in 15 epochs. The basic backbone is input block -> 2 conv blocks (conv + ReLU) -> transition block (2x2 maxpool) -> 4 conv blocks (conv + ReLU) -> final conv2d layer. We haven't used any batchnorms or LR Scheduler. We set the LR to 0.01 with SGD (used in class example directly).

[Step 2](https://github.com/raghuch/EVA8_assignments/blob/main/assignment4/EVA4S5F3_step2.ipynb): again re-uses a notebook shared in the class. now trying to get the model size down first. The LR = 0.01 with SGD as in step 1 and no shceduler is used. The change is in the backbone CNN which goes like this:      input block -> 2 conv blocks (conv + ReLU) -> transition block (2x2 maxpool) -> 4 conv blocks (conv + ReLU) -> final conv2d layer. The difference from Step 1 is the number of channels: Step1 has a channel progression like 1 -> 32 -> 64 -> 128 -> 32 -> 64 -> 128 -> 10 -> 10,  where as we don't cross 20 channels in any conv2d layer here. Both used a large 7x7 conv in the final layer. Number of params is nearly 10.8k (slightly above threshold), and the test accuracy for 15 epochs: 98.53% and for 20 epochs: 98.47. Clearly, decreasing model size alone hurts the test accuracy, but it is still close to Step-1

[Step 3](https://github.com/raghuch/EVA8_assignments/blob/main/assignment4/EVA4S5F4_step3.ipynb): We largely follow Step-2 in spirit. The only change is the introduction of BatchNorm layers so most of the conv block looks like: conv2d -> BatchNorm2d -> ReLU, except the last conv2d layer. Model size is still almost 10.8k, LR and SGD optimizer were left as-is. The test accuracy for 15 epochs: 99.15% and for 20 epochs: 99.21% . It shows the usefulness of BatchNorm layer, but optimizer params are not helping us to converge fast.

[Step 4](https://github.com/raghuch/EVA8_assignments/blob/main/assignment4/EVA4S5F10_step4.ipynb): We added some augmentations: Random rotation of 7degrees, following the example from class. We could add larger angles, but fearing it might take longer to converge to required accuracy, I haven't done that. Another change is the removal of one conv block just before the GAP layer, as an experiment to reduce model size, now 9600 params. I selected that particular conv block since the number of input and output channels are the same, so I thought I could remove since number of input and output features is the same. (May work for smaller datasets like MNIST, but may fail on larger datasets since many more features are present in larger datasets). No change in LR or SGD, and we get test accuracy for 15 epochs: 99.38% and for 20 epochs 99.24%. The test accuracy is wavering around 99.3% so LR might be the cause that we are over-shooting the optimal values.

[Step 5](https://github.com/raghuch/EVA8_assignments/blob/main/assignment4/EVA4S5F10_step5.ipynb): The augmnetations and the CNN backbone remain the same as in step 4, so we are still at 9,600 params. The only change I have done is tweak the LR and the scheduler. I changed the LR to 0.05 from 0.01 (increased 5x), and the shceduler with step size 6 and gamma 0.25 _i.e._ every 6 steps the LR is multipled the gamma (0.25). The intent behind this is that initially the LR is high, so we get to a high test accuracy fast (reached 99% by epoch 3 but fluctuating). After 6 epochs, we quarter the LR so it will stay close to the 99%. Finally epoch 12, epoch 14, and epoch 15 show test accuracies of 99.4% with other epochs also above 99.3%.

(P.S.: Ran Step 5 till 20 epochs with same seed and have observed accuracy still above 99.4% between 16th and 20th epochs, but didn't include since the target is to achieve 99.4% in 15 epochs)
